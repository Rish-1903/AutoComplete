{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c132682-d4ca-4e6a-9cb8-3d0b9db72ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rishabh/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 138771\n",
      "First 300 letters of the data\n",
      "-------\n",
      "How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\n",
      "When you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\n",
      "they've decided its more fun if I don't.\n",
      "So Tired D; Played Lazer Tag & Ran A \n",
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n",
      "..one of my most high traffic blog posts ever for sure\n",
      "Woke up to 70 followers ( in 2 days) #blessed Thanks for ur support!\n",
      "can't wait for your show in Dallas tonight! We'll be there with bells on, so to speak :)\n",
      "Thanks for the follow Kevin!\n",
      "Hm. When you don't know if an indirect tweets about you <<\n",
      "-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "with open(\"./en_US.twitter.txt\",\"r\") as f:\n",
    "    data=f.read()\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "print(data[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "print(data[-300:])\n",
    "print(\"-------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c459cc5-a1a9-410d-ba48-ab28f9b5db65",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7982e01-a0fb-4014-8376-499841caeab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    sentences=data.split(\"\\n\")\n",
    "    sentences=[s.strip() for s in sentences]\n",
    "    sentences=[s for s in sentences if len(s)>0]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d9b3ae-de71-4aa3-b7fe-8c1d55b1183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I have a pen.\n",
      "I have an apple. \n",
      "Ah\n",
      "Apple pen.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"\"\"\n",
    "I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n",
    "\"\"\"\n",
    "print(x)\n",
    "\n",
    "split_to_sentences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e772d576-6a6d-4028-9280-e9829816fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences=[]\n",
    "    for sentence in sentences:\n",
    "        sentence=sentence.lower()\n",
    "        tokenized=nltk.word_tokenize(sentence)\n",
    "        tokenized_sentences.append(tokenized)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8fa955-1884-4ac9-8b88-c1a1b4a89547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sky', 'is', 'blue', '.'],\n",
       " ['leaves', 'are', 'green', '.'],\n",
       " ['roses', 'are', 'red', '.']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
    "tokenize_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51c1a8c2-854b-453f-b14e-f6e74f4a89c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    sentences=split_to_sentences(data)\n",
    "    tokenized_sentences=tokenize_sentences(sentences)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1919372e-dd83-4b9e-bcc9-aef9d82ec3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    word_counts={}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            if token not in word_counts:\n",
    "                word_counts[token]=1\n",
    "            else:\n",
    "                word_counts[token]+=1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0f7d61-fd37-4b34-b5e8-528f5edb2ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sky': 1,\n",
       " 'is': 1,\n",
       " 'blue': 1,\n",
       " '.': 3,\n",
       " 'leaves': 1,\n",
       " 'are': 2,\n",
       " 'green': 1,\n",
       " 'roses': 1,\n",
       " 'red': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "count_words(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ecae5d6-00df-4714-9c0e-e51f94fb4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences,count_threshold):\n",
    "    closed_vocab=[]\n",
    "    word_counts=count_words(tokenized_sentences)\n",
    "    for word,cnt in word_counts.items():\n",
    "        if cnt>=count_threshold:\n",
    "            closed_vocab.append(word)\n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8631b984-2d17-4379-8447-9acc709b5168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed vocabulary:\n",
      "['.', 'are']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
    "print(f\"Closed vocabulary:\")\n",
    "print(tmp_closed_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "275f75bb-e81c-4f17-9b5d-9c344d9398c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences,vocabulary,unknown_token=\"<unk>\"):\n",
    "    vocabulary=set(vocabulary)\n",
    "    replaced_tokenized_sentence=[]\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence=[]\n",
    "        for token in sentence:\n",
    "            if token in vocabulary:\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        replaced_tokenized_sentence.append(replaced_sentence)\n",
    "    return replaced_tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96ed2459-c980-420e-9424-1cb78cd27d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "[['dogs', 'run'], ['cats', 'sleep']]\n",
      "tokenized_sentences with less frequent words converted to '<unk>':\n",
      "[['dogs', '<unk>'], ['<unk>', 'sleep']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\n",
    "vocabulary = [\"dogs\", \"sleep\"]\n",
    "tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\n",
    "print(f\"Original sentence:\")\n",
    "print(tokenized_sentences)\n",
    "print(f\"tokenized_sentences with less frequent words converted to '<unk>':\")\n",
    "print(tmp_replaced_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11f93f90-65c4-4bc1-9e40-31ca6424f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold, unknown_token=\"<unk>\",\n",
    "                    get_words_with_nplus_frequency=get_words_with_nplus_frequency, replace_oov_words_by_unk=replace_oov_words_by_unk):\n",
    "    vocabulary=get_words_with_nplus_frequency(train_data,count_threshold)\n",
    "    train_data_replaced=replace_oov_words_by_unk(train_data,vocabulary,unknown_token)\n",
    "    test_data_replaced=replace_oov_words_by_unk(test_data,vocabulary,unknown_token)\n",
    "    return train_data_replaced,test_data_replaced,vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7701880-3208-43da-8dcd-d337c5b7d0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_train_repl\n",
      "[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]\n",
      "\n",
      "tmp_test_repl\n",
      "[['<unk>', 'are', '<unk>', '.']]\n",
      "\n",
      "tmp_vocab\n",
      "['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "tmp_train = [['sky', 'is', 'blue', '.'],\n",
    "     ['leaves', 'are', 'green']]\n",
    "tmp_test = [['roses', 'are', 'red', '.']]\n",
    "\n",
    "tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, \n",
    "                                                           tmp_test, \n",
    "                                                           count_threshold = 1\n",
    "                                                          )\n",
    "\n",
    "print(\"tmp_train_repl\")\n",
    "print(tmp_train_repl)\n",
    "print()\n",
    "print(\"tmp_test_repl\")\n",
    "print(tmp_test_repl)\n",
    "print()\n",
    "print(\"tmp_vocab\")\n",
    "print(tmp_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04a7374e-9d2a-4bc2-bcd4-2debfb942d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = get_tokenized_data(data)\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]\n",
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d4f12c2-f1dd-4e09-87b0-19f6c207b3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First preprocessed training sample:\n",
      "['it', \"'s\", '<unk>', 'again', '..', 'but', 'oh', 'well']\n",
      "\n",
      "First preprocessed test sample:\n",
      "['big', '<unk>', 'to', 'for', 'leaving', '<unk>', 'that', 'google', 'can', '<unk>', '<unk>', '.']\n",
      "\n",
      "First 10 vocabulary:\n",
      "['it', \"'s\", 'again', '..', 'but', 'oh', 'well', '``', 'them', 'the']\n",
      "\n",
      "Size of vocabulary: 1702\n"
     ]
    }
   ],
   "source": [
    "print(\"First preprocessed training sample:\")\n",
    "print(train_data_processed[0])\n",
    "print()\n",
    "print(\"First preprocessed test sample:\")\n",
    "print(test_data_processed[0])\n",
    "print()\n",
    "print(\"First 10 vocabulary:\")\n",
    "print(vocabulary[0:10])\n",
    "print()\n",
    "print(\"Size of vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a081d0-bed8-4350-9fa1-4a39df8528b1",
   "metadata": {},
   "source": [
    "## Develop_N_gram_Language_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed32f461-58d0-4d53-8136-8781b7dc98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data,n,start_token=\"<s>\",end_token=\"<e>\"):\n",
    "    n_grams={}\n",
    "    for sentence in data:\n",
    "        sentence=[start_token]*n+sentence+[end_token]\n",
    "        sentence=tuple(sentence)\n",
    "        for i in range(len(sentence)-n+1):\n",
    "            n_gram=sentence[i:i+n]\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[n_gram]+=1\n",
    "            else:\n",
    "                n_grams[n_gram]=1\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "938351d5-57bc-4de0-a48e-c3b328bb1b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3210cf55-e8d4-4a99-b159-801a1888a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word,previous_n_gram,n_gram_counts,n_plus1_gram_counts,vocabulary_size,k=1.0):\n",
    "    previous_n_gram=tuple(previous_n_gram)\n",
    "    previous_n_gram_count=n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "    denominator=previous_n_gram_count+k*vocabulary_size\n",
    "    n_plus1_gram=previous_n_gram+(word,)\n",
    "    n_plus_1_gram_count=n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n",
    "    numerator=n_plus_1_gram_count+k\n",
    "    probability=numerator/denominator\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f65a897d-1372-44e5-a41b-f93226424e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "tmp_prob = estimate_probability(\"cat\", [\"a\"], unigram_counts, bigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8b3100c-22ed-47f2-bb18-708c04468b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\",  k=1.0):\n",
    "    previous_n_gram=tuple(previous_n_gram)\n",
    "    vocabulary=vocabulary+[end_token,unknown_token]\n",
    "    vocabulary_size=len(vocabulary)\n",
    "    probabilities={}\n",
    "    for word in vocabulary:\n",
    "        probability=estimate_probability(word,previous_n_gram,n_gram_counts,n_plus1_gram_counts,vocabulary_size,k=k)\n",
    "        probabilities[word]=probability\n",
    "    return probabilities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2613497a-a34a-494b-bc87-4a10d88bb690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0.09090909090909091,\n",
       " 'is': 0.09090909090909091,\n",
       " 'like': 0.09090909090909091,\n",
       " 'cat': 0.2727272727272727,\n",
       " 'dog': 0.09090909090909091,\n",
       " 'this': 0.09090909090909091,\n",
       " 'a': 0.09090909090909091,\n",
       " '<e>': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "estimate_probabilities([\"a\"], unigram_counts, bigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5f6efff-d8eb-450d-abbd-2a9ba0b8aab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0.18181818181818182,\n",
       " 'is': 0.09090909090909091,\n",
       " 'like': 0.09090909090909091,\n",
       " 'cat': 0.09090909090909091,\n",
       " 'dog': 0.09090909090909091,\n",
       " 'this': 0.18181818181818182,\n",
       " 'a': 0.09090909090909091,\n",
       " '<e>': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "estimate_probabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0549557b-cbfb-48da-851a-954a3e6e672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "    # add <e> <unk> to the vocabulary\n",
    "    # <s> is omitted since it should not appear as the next word\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    \n",
    "    # obtain unique n-grams\n",
    "    n_grams = []\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]        \n",
    "        n_grams.append(n_gram)\n",
    "    n_grams = list(set(n_grams))\n",
    "    \n",
    "    # mapping from n-gram to row\n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}    \n",
    "    # mapping from next word to column\n",
    "    col_index = {word:j for j, word in enumerate(vocabulary)}    \n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c8f8b9f-c147-40c6-ab4e-7a4ad766189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>this</th>\n",
       "      <th>a</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           i   is  like  cat  dog  this    a  <e>  <unk>\n",
       "(<s>,)   1.0  0.0   0.0  0.0  0.0   1.0  0.0  0.0    0.0\n",
       "(dog,)   0.0  1.0   0.0  0.0  0.0   0.0  0.0  0.0    0.0\n",
       "(a,)     0.0  0.0   0.0  2.0  0.0   0.0  0.0  0.0    0.0\n",
       "(i,)     0.0  0.0   1.0  0.0  0.0   0.0  0.0  0.0    0.0\n",
       "(is,)    0.0  0.0   1.0  0.0  0.0   0.0  0.0  0.0    0.0\n",
       "(this,)  0.0  0.0   0.0  0.0  1.0   0.0  0.0  0.0    0.0\n",
       "(like,)  0.0  0.0   0.0  0.0  0.0   0.0  2.0  0.0    0.0\n",
       "(cat,)   0.0  0.0   0.0  0.0  0.0   0.0  0.0  2.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "print('bigram counts')\n",
    "display(make_count_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3555a16-194f-475d-81c8-44a0c670be6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>this</th>\n",
       "      <th>a</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               i   is  like  cat  dog  this    a  <e>  <unk>\n",
       "(<s>, <s>)   1.0  0.0   0.0  0.0  0.0   1.0  0.0  0.0    0.0\n",
       "(this, dog)  0.0  1.0   0.0  0.0  0.0   0.0  0.0  0.0    0.0\n",
       "(a, cat)     0.0  0.0   0.0  0.0  0.0   0.0  0.0  2.0    0.0\n",
       "(like, a)    0.0  0.0   0.0  2.0  0.0   0.0  0.0  0.0    0.0\n",
       "(<s>, i)     0.0  0.0   1.0  0.0  0.0   0.0  0.0  0.0    0.0\n",
       "(<s>, this)  0.0  0.0   0.0  0.0  1.0   0.0  0.0  0.0    0.0\n",
       "(i, like)    0.0  0.0   0.0  0.0  0.0   0.0  1.0  0.0    0.0\n",
       "(is, like)   0.0  0.0   0.0  0.0  0.0   0.0  1.0  0.0    0.0\n",
       "(dog, is)    0.0  0.0   1.0  0.0  0.0   0.0  0.0  0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('\\ntrigram counts')\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "display(make_count_matrix(trigram_counts, unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2cfc783-2f89-4e4b-804d-01cb2b89d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e5d4b23-0569-4f1b-a4ca-350f085fa599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>this</th>\n",
       "      <th>a</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                i        is      like       cat       dog      this         a  \\\n",
       "(<s>,)   0.181818  0.090909  0.090909  0.090909  0.090909  0.181818  0.090909   \n",
       "(dog,)   0.100000  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(a,)     0.090909  0.090909  0.090909  0.272727  0.090909  0.090909  0.090909   \n",
       "(i,)     0.100000  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(is,)    0.100000  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(this,)  0.100000  0.100000  0.100000  0.100000  0.200000  0.100000  0.100000   \n",
       "(like,)  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.272727   \n",
       "(cat,)   0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "\n",
       "              <e>     <unk>  \n",
       "(<s>,)   0.090909  0.090909  \n",
       "(dog,)   0.100000  0.100000  \n",
       "(a,)     0.090909  0.090909  \n",
       "(i,)     0.100000  0.100000  \n",
       "(is,)    0.100000  0.100000  \n",
       "(this,)  0.100000  0.100000  \n",
       "(like,)  0.090909  0.090909  \n",
       "(cat,)   0.272727  0.090909  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "print(\"bigram probabilities\")\n",
    "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b2394b-185d-41f6-b930-2a203d6307de",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c03e4fe-7aff-47c0-8472-ef19c907b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, start_token='<s>', end_token = '<e>', k=1.0):\n",
    "    n=len(list(n_gram_counts.keys())[0])\n",
    "    sentence=[start_token]*n+sentence+[end_token]\n",
    "    sentence=tuple(sentence)\n",
    "    N=len(sentence)\n",
    "    product_pi=1.0\n",
    "    for t in range(n,N):\n",
    "        n_gram=sentence[t-n:t]\n",
    "        word=sentence[t]\n",
    "        probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
    "        product_pi*=1/probability\n",
    "    perplexity=(product_pi)**(1/N)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8319d3e-101a-4785-bab1-a185ef2dd89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for first train sample: 2.8040\n",
      "Perplexity for test sample: 3.9654\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "\n",
    "perplexity_train = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for first train sample: {perplexity_train:.4f}\")\n",
    "\n",
    "test_sentence = ['i', 'like', 'a', 'dog']\n",
    "perplexity_test = calculate_perplexity(test_sentence,\n",
    "                                       unigram_counts, bigram_counts,\n",
    "                                       len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c584607-de57-49dc-ab09-32cbaa797d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0, start_with=None):\n",
    "    n=len(list(n_gram_counts.keys())[0])\n",
    "    previous_n_gram=previous_tokens[-n:]\n",
    "    probabilities=estimate_probabilities(previous_n_gram,n_gram_counts,n_plus1_gram_counts,vocabulary,k=k)\n",
    "    suggestion=None\n",
    "    max_prob=0\n",
    "    for word,prob in probabilities.items():\n",
    "        if start_with:\n",
    "            if not word.startswith(start_with):\n",
    "                continue\n",
    "        if prob>max_prob:\n",
    "            suggestion=word\n",
    "            max_prob=prob\n",
    "    return suggestion,max_prob         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49891b92-c470-4bac-8543-129a4e29d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like',\n",
      "\tand the suggested word is `a` with a probability of 0.2727\n",
      "\n",
      "The previous words are 'i like', the suggestion must start with `c`\n",
      "\tand the suggested word is `cat` with a probability of 0.0909\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
    "\n",
    "print()\n",
    "# test your code when setting the starts_with\n",
    "tmp_starts_with = 'c'\n",
    "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
    "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "827f19a3-8fae-4570-8962-c8c23edd7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d891d5b8-749a-411f-b936-5cfc63587b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like', the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 0.2727272727272727),\n",
       " ('a', 0.2),\n",
       " ('i', 0.1111111111111111),\n",
       " ('i', 0.1111111111111111)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "quadgram_counts = count_n_grams(sentences, 4)\n",
    "qintgram_counts = count_n_grams(sentences, 5)\n",
    "\n",
    "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
    "\n",
    "print(f\"The previous words are 'i like', the suggestions are:\")\n",
    "display(tmp_suggest3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33ec96d5-9943-4c75-a65f-ec2e72ef12e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sequence of words:  I love\n",
      "Optional: Enter a starting letter or sequence for the suggested word (or press Enter to skip):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed Sentence:\n",
      "I love you\n"
     ]
    }
   ],
   "source": [
    "# Assuming the necessary imports, n_gram_counts_list, and vocabulary have been defined earlier\n",
    "\n",
    "# Function to get user input\n",
    "def get_user_input():\n",
    "    user_input = input(\"Enter a sequence of words: \")\n",
    "    previous_tokens = user_input.split()  # Split the input string into individual tokens (words)\n",
    "    return previous_tokens\n",
    "\n",
    "# Function to get and display suggestions based on user input\n",
    "def suggest_and_complete_sentence(n_gram_counts_list, vocabulary, k=1.0):\n",
    "    previous_tokens = get_user_input()\n",
    "    \n",
    "    start_with = input(\"Optional: Enter a starting letter or sequence for the suggested word (or press Enter to skip): \")\n",
    "    start_with = start_with if start_with != \"\" else None\n",
    "    \n",
    "    # Get word suggestions\n",
    "    suggestions = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=k, start_with=start_with)\n",
    "    \n",
    "    # Find the suggestion with the highest probability\n",
    "    if suggestions:\n",
    "        best_suggestion, max_prob = max(suggestions, key=lambda x: x[1])\n",
    "        if best_suggestion:\n",
    "            # Append the best suggestion to the previous tokens\n",
    "            completed_sentence = previous_tokens + [best_suggestion]\n",
    "            print(\"\\nCompleted Sentence:\")\n",
    "            print(\" \".join(completed_sentence))\n",
    "        else:\n",
    "            print(\"No suitable suggestion found.\")\n",
    "    else:\n",
    "        print(\"No suggestions available.\")\n",
    "\n",
    "# Call the function to suggest and complete the sentence based on user input\n",
    "suggest_and_complete_sentence(n_gram_counts_list, vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe45c0-c4b8-4875-b025-176dc62a75c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
